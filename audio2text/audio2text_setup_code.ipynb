{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Build Conda Environment\n",
    "\n",
    "``` bash\n",
    "conda create --name demosnowparkdemo --override-channels -c https://repo.anaconda.com/pkgs/snowflake python=3.8\n",
    "conda activate demosnowparkdemo\n",
    "conda install snowflake-snowpark-python pandas pyarrow streamlit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup - Create Database/Security/Compute Pool objects \n",
    "\n",
    "- Ensure you have followed the steps in the doc https://docs.snowflake.com/en/developer-guide/snowpark-container-services/tutorials/common-setup#create-snowflake-objects to create required roles need for SPCS services. \n",
    "\n",
    "If you haven't already done it, run the below commands in snowsight as <b>ACCOUNTADMIN</b>\n",
    "\n",
    "You have to execute the below commands only once for all the three containers you will be creating as part of the solution. You will be creating the other pools while crearting other SPCS services.\n",
    "\n",
    "``` sql\n",
    "\n",
    "--DROP DATABASE LLMDEMO;\n",
    "--DROP COMPUTE POOL PR_GPU_S;\n",
    "--DROP WAREHOUSE small_warehouse;\n",
    "--DROP SECURITY INTEGRATION snowservices_ingress_oauth;\n",
    "--DROP ROLE SPCS_PSE_ROLE;\n",
    "--DROP EXTERNAL ACCESS INTEGRATION allow_all_eai;\n",
    "\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "CREATE OR REPLACE ROLE SPCS_PSE_ROLE;\n",
    "CREATE OR REPLACE DATABASE LLMDemo;\n",
    "\n",
    "CREATE OR REPLACE WAREHOUSE small_warehouse WITH\n",
    "  WAREHOUSE_SIZE='X-SMALL';\n",
    "GRANT USAGE ON WAREHOUSE small_warehouse TO ROLE SPCS_PSE_ROLE;\n",
    "\n",
    "CREATE SECURITY INTEGRATION IF NOT EXISTS snowservices_ingress_oauth\n",
    "  TYPE=oauth\n",
    "  OAUTH_CLIENT=snowservices_ingress\n",
    "  ENABLED=true;\n",
    "\n",
    "GRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE SPCS_PSE_ROLE;\n",
    "\n",
    "CREATE COMPUTE POOL PR_GPU_S\n",
    "MIN_NODES = 1 \n",
    "MAX_NODES = 1 \n",
    "INSTANCE_FAMILY = GPU_NV_S \n",
    "AUTO_RESUME = FALSE\n",
    "COMMENT = 'For Audio2text' ;\n",
    "\n",
    "-- Below network rule and External Access INtegration is used to download the whisper mode.\n",
    "\n",
    "-- You need to execute the below two commands only once for all the SPC services that you will be creating.\n",
    "\n",
    "CREATE NETWORK RULE allow_all_rule\n",
    "    TYPE = 'HOST_PORT'\n",
    "    MODE= 'EGRESS'\n",
    "    VALUE_LIST = ('0.0.0.0:443','0.0.0.0:80');\n",
    "\n",
    "CREATE EXTERNAL ACCESS INTEGRATION allow_all_eai\n",
    "  ALLOWED_NETWORK_RULES = (allow_all_rule)\n",
    "  ENABLED = true;\n",
    "\n",
    "GRANT USAGE ON INTEGRATION allow_all_eai TO ROLE SPCS_PSE_ROLE;\n",
    "\n",
    "GRANT USAGE, MONITOR ON COMPUTE POOL PR_GPU_S TO ROLE SPCS_PSE_ROLE;\n",
    "\n",
    "GRANT USAGE, MONITOR ON COMPUTE POOL PR_GPU_S TO ROLE SPCS_PSE_ROLE;\n",
    "\n",
    "GRANT DATABASE ROLE SNOWFLAKE.CORTEX_USER TO ROLE SPCS_PSE_ROLE;\n",
    "\n",
    "GRANT ROLE SPCS_PSE_ROLE TO USER psheehan;\n",
    "\n",
    "GRANT OWNERSHIP ON DATABASE LLMDemo TO ROLE SPCS_PSE_ROLE COPY CURRENT GRANTS;\n",
    "\n",
    "GRANT OWNERSHIP ON ALL SCHEMAS IN DATABASE LLMDemo  TO ROLE SPCS_PSE_ROLE COPY CURRENT GRANTS;\n",
    "\n",
    "GRANT DATABASE ROLE SNOWFLAKE.CORTEX_USER TO ROLE SPCS_PSE_ROLE;\n",
    "\n",
    "USE ROLE SPCS_PSE_ROLE;\n",
    "USE DATABASE LLMDemo;\n",
    "USE WAREHOUSE small_warehouse;\n",
    "USE SCHEMA PUBLIC;\n",
    "\n",
    "CREATE IMAGE REPOSITORY IF NOT EXISTS IMAGES;\n",
    "\n",
    "-- CHECK THE IMAGE RESGITRY URL\n",
    "\n",
    "SHOW IMAGE REPOSITORIES;\n",
    "\n",
    "--Example output for the above query:\n",
    "-- <orgname>-<acctname>.registry.snowflakecomputing.com/LLMDEMO/public/images\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build docker image and push the image to image registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Run below commands from a terminal. Ensure your docker is running on your laptop. Update the ORGNAME-ACCTNAME with your Snowflake account info and also update username \n",
    "\n",
    "``` bash\n",
    "\n",
    "cd audio2text\n",
    "\n",
    "-- Refer audio2text/Dockerfile for image details\n",
    "\n",
    "docker build --no-cache --platform linux/amd64 -t ORGNAME-ACCTNAME.registry.snowflakecomputing.com/llmdemo/public/images/whisper-audio2text:latest . \n",
    "\n",
    "-- username and password is same as your snowflake credential\n",
    "\n",
    "docker login ORGNAME-ACCTNAME.registry.snowflakecomputing.com -u <username> -p <password>\n",
    "\n",
    "docker push ORGNAME-ACCTNAME.registry.snowflakecomputing.com/llmdemo/public/images/whisper-audio2text:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating Internal Stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection.json file should use the SPCS_PSE_ROLE which you have created earlier\n",
    "\n",
    "connection_parameters = json.load(open('../connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the below command to create the required stage\n",
    "stages=['WHISPER_APP','AUDIO_FILES','SPECS','CSV_FILES']\n",
    "for stg in stages:\n",
    "    session.sql(f'''\n",
    "                CREATE OR REPLACE STAGE {stg} ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') \n",
    "                DIRECTORY = (ENABLE = TRUE);\n",
    "                ''').collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. Create SPC Service\n",
    "Update th YAML details [whisper_spec.yml](./whisper_spec.yml) to change the image url before executing the below put command\n",
    "\n",
    "image: ORGNAME-ACCTNAME.registry.snowflakecomputing.com/pr_llmdemo/public/image_repo/whisper-audio2text:latest\n",
    "\n",
    "PS: <b>Run all the below commands using the SPCS Role(or any custom role you have created) and not using accountadmin </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session.file.put(\"./whisper_spec.yml\", \"@specs\",auto_compress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the service\n",
    "session.sql('''\n",
    "CREATE SERVICE IF NOT EXISTS Whisper_Audio_text_SVC\n",
    "  IN COMPUTE POOL PR_GPU_S\n",
    "  FROM @specs\n",
    "  SPEC='whisper_spec.yml'\n",
    "  EXTERNAL_ACCESS_INTEGRATIONS = (ALLOW_ALL_EAI)\n",
    "  MIN_INSTANCES=1\n",
    "  MAX_INSTANCES=1;\n",
    "            ''').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The service must be in Ready State to proceed. Run the following command to confirm before proceeding to next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service activation may take a few minutes. Now is a good time for a bio break. ;)\n",
    "import ast\n",
    "res=session.sql(''' \n",
    "SELECT SYSTEM$GET_SERVICE_STATUS('Whisper_Audio_text_SVC',1)\n",
    "''').collect()[0][0]\n",
    "ast.literal_eval(res)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check the log for the service for any errors.\n",
    "session.sql('''SELECT value AS log_line\n",
    "FROM TABLE(\n",
    " SPLIT_TO_TABLE(SYSTEM$GET_SERVICE_LOGS('Whisper_Audio_text_SVC', 0, 'audio-whisper-app'), '\\n')\n",
    "  )''').to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating the service function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Function to get duration of the audio files\n",
    "session.sql('''CREATE OR REPLACE FUNCTION DURATION(AUDIO_FILE TEXT)\n",
    "RETURNS VARIANT\n",
    "SERVICE=Whisper_Audio_text_SVC\n",
    "ENDPOINT=API\n",
    "AS '/audio-duration'\n",
    "            ''').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe the audio files\n",
    "session.sql('''CREATE OR REPLACE FUNCTION TRANSCRIBE(TASK TEXT, LANGUAGE TEXT, AUDIO_FILE TEXT, ENCODE BOOLEAN)\n",
    "RETURNS VARIANT\n",
    "SERVICE=Whisper_Audio_text_SVC\n",
    "ENDPOINT=API\n",
    "AS '/asr'\n",
    "            ''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language of the audio file\n",
    "session.sql('''CREATE OR REPLACE FUNCTION DETECT_LANGUAGE(AUDIO_FILE TEXT, ENCODE BOOLEAN)\n",
    "RETURNS VARIANT\n",
    "SERVICE=Whisper_Audio_text_SVC\n",
    "ENDPOINT=API\n",
    "AS '/detect-language'\n",
    "            ''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Table to load the Audio file raw text along with duration and other attributes\n",
    "\n",
    "# Duration is in seconds\n",
    "\n",
    "session.sql('''\n",
    "    CREATE or REPLACE TABLE ALL_CLAIMS_RAW (\n",
    "\tDATETIME DATE,\n",
    "\tAUDIOFILE VARCHAR(16777216),\n",
    "\tCONVERSATION VARCHAR(16777216),\n",
    "\tPRESIGNED_URL_PATH VARCHAR(16777216),\n",
    "\tDURATION FLOAT NOT NULL\n",
    ")''').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Uploading the audio files to Internal Stage\n",
    "\n",
    "_ = session.file.put(\"./audiofiles/*.*\", \"@AUDIO_FILES/2024-01-26/\", auto_compress=False,overwrite=True)\n",
    "\n",
    "session.sql(f'''ALTER STAGE AUDIO_FILES REFRESH''').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('ls @AUDIO_FILES/2024-01-26').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting records into the RAW Table\n",
    "# To have different values for the datetime, store your audio files in sub folders with yyy-mm-dd format . \n",
    "# E.g. 2024-01-10. \n",
    "session.sql('''\n",
    "INSERT INTO ALL_CLAIMS_RAW\n",
    "(\n",
    "DATETIME,\n",
    "AUDIOFILE,\n",
    "PRESIGNED_URL_PATH,\n",
    "CONVERSATION,\n",
    "DURATION\n",
    ")\n",
    "SELECT CAST(CASE WHEN split(RELATIVE_PATH,'/')[1]::string IS NULL THEN GETDATE() \n",
    "            ELSE split(RELATIVE_PATH,'/')[0]::string END AS DATE) as Datetime, \n",
    "        CASE WHEN split(RELATIVE_PATH,'/')[1]::string is null then split(RELATIVE_PATH,'/')[0]::string \n",
    "            ELSE split(RELATIVE_PATH,'/')[1]::string END as RELATIVE_PATH,\n",
    "       GET_PRESIGNED_URL('@AUDIO_FILES', RELATIVE_PATH) AS PRESIGNED_URL\n",
    "       -- ,DETECT_LANGUAGE(PRESIGNED_URL,TRUE) as DETECT_LANGUAGE\n",
    "       ,TRANSCRIBE('transcribe','',PRESIGNED_URL,True)['text']::string AS EXTRACTED_TEXT\n",
    "       ,DURATION(PRESIGNED_URL):call_duration_seconds::DOUBLE as CALL_DURATION_SECONDS\n",
    "FROM DIRECTORY('@AUDIO_FILES')\n",
    "            \n",
    "            ''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table('ALL_CLAIMS_RAW').to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Loading Data into the ALL_CLAIMS_RAW Table from CSV\n",
    "\n",
    "Since we don't have lot of audio files from insurance industry, we will be loading sample data into the Raw table which has the raw conversation from the insurance industry. This data will be the source for this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = session.file.put(\"./Sample_Audio_Text.csv\", \"@CSV_FILES\", auto_compress=False)\n",
    "\n",
    "sp_df=session.read.options({\"INFER_SCHEMA\":True,\"PARSE_HEADER\":True,\"FIELD_OPTIONALLY_ENCLOSED_BY\":'\"'}).csv('@CSV_FILES/Sample_Audio_Text.csv')\n",
    "\n",
    "# sp_df = session.read.option(\"INFER_SCHEMA\", True).option(\"PARSE_HEADER\", True).option(\"FIELD_OPTIONALLY_ENCLOSED_BY\",'\"').csv(\"@CSV_FILES/Sample_Audio_Text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df.write.mode(\"overwrite\").save_as_table(\"ALL_CLAIMS_RAW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table('ALL_CLAIMS_RAW').to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark_3_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
